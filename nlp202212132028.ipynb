{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: S08920611 & S08920030 from Sustainability Science and Management Dept., Tunghai University\n",
    "Please get data from gisaid-variants-statistics at gisaid.org !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,tarfile,math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.dom.minidom\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def Extract_file(file_path):  #Extract downloaded file from GISAID\n",
    "    print(\"---Extract---\")\n",
    "    if os.path.isfile(file_path):\n",
    "        tar=tarfile.open(file_path,'r:xz')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        print(\"Extract finish.\")\n",
    "    elif file_path=='skip':\n",
    "        print(\"Extract part skipped.\")\n",
    "    else:\n",
    "        print(\"Error: file does not exist, please check the path.\")\n",
    "def Preprocess():  #Data preprocess\n",
    "    print(\"---Data preprocess---\")\n",
    "    df = pd.read_json(\"gisaid_variants_statistics.json\",encoding=\"utf-8\", orient='records')\n",
    "    global db\n",
    "    db=pd.DataFrame(index=range(df.shape[0]),columns=['Time', 'Species', 'AA_Substitution', 'Lineage'])\n",
    "    dataset=[]\n",
    "    db=pd.DataFrame(columns=['Time', 'Species', 'AA_Substitution', 'Lineage'])\n",
    "    df=df.rename_axis('time').reset_index()\n",
    "    global spikeset  #Collect all spike variants\n",
    "    spikeset=set()\n",
    "    for i in range(df.index[-1]):\n",
    "        country=str(df.iloc[i][\"stats\"])[2:str(df.iloc[i][\"stats\"]).find(\":\")-1]\n",
    "        df.iloc[i][\"stats\"][country]\n",
    "        db.loc[len(db.index)]=(int(str(df.iloc[i][\"time\"])[:4])-2010)*365+int(str(df.iloc[i][\"time\"])[5:7])*30+int(str(df.iloc[i][\"time\"])[8:10]),df.iloc[i][\"stats\"][country]['submissions_per_variant'][0]['value'],df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'],df.iloc[i][\"stats\"][country]['submissions_per_lineage']\n",
    "        if len(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'])>1:\n",
    "            db.loc[i,'AA_Substitution']=['']\n",
    "            for j in range(int(len(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution']))):\n",
    "                db.iloc[i]['AA_Substitution'].append(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'][j]['value'])\n",
    "                spikeset.add(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'][j]['value'])\n",
    "            db.loc[i,'AA_Substitution'].remove('')\n",
    "        elif len(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'])==1:\n",
    "            db.loc[i,'AA_Substitution']=['']\n",
    "            db.iloc[i]['AA_Substitution'].append(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'][0]['value'])\n",
    "            db.loc[i,'AA_Substitution'].remove('')\n",
    "            spikeset.add(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'][0]['value'])\n",
    "        if len(df.iloc[i][\"stats\"][country]['submissions_per_lineage'])>1:\n",
    "            db.loc[i,'Lineage']=['']\n",
    "            for j in range(int(len(df.iloc[i][\"stats\"][country]['submissions_per_lineage']))):\n",
    "                db.iloc[i]['Lineage'].append(df.iloc[i][\"stats\"][country]['submissions_per_lineage'][j]['value'])\n",
    "            db.loc[i,'Lineage'].remove('')\n",
    "        elif len(df.iloc[i][\"stats\"][country]['submissions_per_lineage'])==1:\n",
    "            db.loc[i,'Lineage']=['']\n",
    "            db.iloc[i]['Lineage'].append(df.iloc[i][\"stats\"][country]['submissions_per_lineage'][0]['value'])\n",
    "            db.loc[i,'Lineage'].remove('')\n",
    "    print(\"Data preprocess done.\")\n",
    "def Model_Training():\n",
    "    print(\"---Model training---\")\n",
    "    y = list(spikeset)\n",
    "    le = LabelEncoder()  #Encode spikes into array instead of spike names\n",
    "    le.fit(y)\n",
    "    ylist=[]\n",
    "    for i in range(db.values.T[2].shape[0]):\n",
    "        ylist.append(list(le.transform(db.values.T[2][i])))\n",
    "    ylistarr=np.zeros((len(ylist),len(list(spikeset))))\n",
    "    for i in range (len(ylist)):\n",
    "        for j in range(len(list(spikeset))):\n",
    "            if j in ylist[i]:\n",
    "                ylistarr[i,j]=1\n",
    "    data_x = db.values.T[0].astype(np.float32).reshape(-1,1)\n",
    "    data_y = ylistarr.astype(np.float32)\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(data_x,data_y,test_size=0.25,random_state=1)\n",
    "    global model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1000,kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=10000,kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=data_y.shape[1],kernel_initializer='normal',activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    train_history=model.fit(x=xtrain,y=ytrain,validation_split=0.2,epochs=50,batch_size=200,verbose=0)\n",
    "    xpred=model.predict(xtest)\n",
    "    accuracy=0\n",
    "    for i in range(ytest.shape[0]):  #Rewrite accuracy function\n",
    "        for j in range(ytest.shape[1]):\n",
    "            if ytest[i][j]==xpred[i][j]:\n",
    "                accuracy+=1\n",
    "    accuracy=accuracy/ytest.shape[0]/ytest.shape[1] \n",
    "    print('accuracy',accuracy)\n",
    "    print(\"Model training done.\")\n",
    "def Predict_Spike():\n",
    "    print(\"---Predict spike---\")\n",
    "    timestr=\"\"\n",
    "    if len(timestr)!=10:  #Enter predicting date \n",
    "        timestr=input(\"Enter the time that you want to predict(Ex: 2023.01.01): \")\n",
    "        if len(timestr)!=10:\n",
    "            print(\"Invalid input, please check and enter again!\")\n",
    "    time=float((int(timestr[:4])-2010)*365+int(timestr[5:7])*30+int(timestr[8:10]))\n",
    "    pred=model.predict(np.array(time).reshape(-1,1))[0]\n",
    "    global spike\n",
    "    spike=[]\n",
    "    for i in range(pred.shape[0]):\n",
    "        if pred[i]==0:\n",
    "            predlog10=-10\n",
    "        else:\n",
    "            predlog10=(math.log10(abs(pred[i])))*-1\n",
    "        if predlog10>-10:\n",
    "            spike.append(list(spikeset)[i])     #Simple predict function(Could replaced by a more convincing function)\n",
    "    print(\"The new virus might contain:\",str(spike)[1:len(str(spike))-1])\n",
    "    print(\"Predict spike done.\")\n",
    "def Predict_Effect():\n",
    "    print(\"---Predict effect---\")\n",
    "    txt=open('Mutations and Evolution of the SARS-CoV-2 Spike Protein.txt','r',encoding=\"utf-8\")\n",
    "    document=txt.read()\n",
    "    global sentence\n",
    "    sentence=[]\n",
    "    for i in range(len(spike)):\n",
    "        if document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])!=-1:\n",
    "            sentence.append(document[document[:document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])].rfind(\".\")+2:document[document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:]):].find(\".\")+document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])])\n",
    "    cutwords5=[]\n",
    "    for i in range (len(sentence)):\n",
    "        cutwords1 = word_tokenize(sentence[i])\n",
    "        interpunctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "        cutwords2 = [word for word in cutwords1 if word not in interpunctuations]\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        cutwords3 = [word for word in cutwords2 if word not in stops]\n",
    "        cutwords4 = []\n",
    "        for cutword in cutwords3:\n",
    "            cutwords4.append(PorterStemmer().stem(cutword))\n",
    "        cutwords5.append(cutwords4)\n",
    "    vocab=[['neutral','diminish','evad','impair','immune-evas','resist','lost-act','escap'],['mouse-adapt','mous'],['infect','bind','viral-load','receptor-bind','receptor','transmiss','spread'],['destabil']]\n",
    "    #List1:Immune escape capability; 2:Zoonosis 3.Rapidly dissemination ability 4.Mutability\n",
    "    Immune_escape_capability=Zoonosis=Rapidly_dissemination_ability=Mutability=count=0\n",
    "    for i in range(len(cutwords5)):\n",
    "        for j in range(len(cutwords5[i])):\n",
    "            if cutwords5[i][j] in vocab[0]:\n",
    "                Immune_escape_capability+=1\n",
    "            if cutwords5[i][j] in vocab[1]:\n",
    "                Zoonosis+=1\n",
    "            if cutwords5[i][j] in vocab[2]:\n",
    "                Rapidly_dissemination_ability+=1\n",
    "            if cutwords5[i][j] in vocab[3]:\n",
    "                Mutability+=1\n",
    "    print(\"The possible property of the new virus is(are): \",end=\"\")\n",
    "    count=0\n",
    "    if Immune_escape_capability>0:\n",
    "        count+=1\n",
    "        print(str(Immune_escape_capability)+\" times of immune escape capability, \",end=\"\")\n",
    "    if Zoonosis>0:\n",
    "        count+=1\n",
    "        print(str(Zoonosis)+\" times of zoonosis, \",end=\"\")\n",
    "    if Rapidly_dissemination_ability>0:\n",
    "        count+=1\n",
    "        print(str(Rapidly_dissemination_ability)+\" times of rapidly dissemination ability, \",end=\"\")\n",
    "    if Mutability>0:\n",
    "        count+=1\n",
    "        print(str(Mutability)+\" times of mutability, \",end=\"\")\n",
    "    if count==0:\n",
    "        print(\"No special property.\")\n",
    "    else:\n",
    "        print(\"than the origin COVID-19.\")\n",
    "    print(\"Predict effect done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Extract---\n",
      "Extract part skipped.\n",
      "---Data preprocess---\n",
      "Data preprocess done.\n",
      "---Model training---\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "accuracy 0.5395121951219513\n",
      "Model training done.\n",
      "---Predict spike---\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "The new virus might contain: 'Spike_N440K', 'Spike_R408S', 'Spike_N460K', 'Spike_L368I', 'Spike_T478K', 'Spike_R346T', 'Spike_K356T', 'Spike_S494P', 'Spike_G446S', 'Spike_T376A', 'Spike_P681H', 'Spike_F486S', 'Spike_N501Y', 'Spike_H69del', 'Spike_G339D', 'Spike_S375F', 'Spike_D614G'\n",
      "Predict spike done.\n",
      "---Predict effect---\n",
      "The possible property of the new virus is(are): 7 times of immune escape capability, 8 times of rapidly dissemination ability, than the origin COVID-19.\n",
      "Predict effect done.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    Extract_file(input(\"Put the data file in the same path, and enter the name of the file(Ex:gisaid_variants_statistics_2022_12_27_1557.tar.xz)(if already extracted, enter ‘skip’): \"))\n",
    "    Preprocess()\n",
    "    Model_Training()\n",
    "    Predict_Spike()\n",
    "    Predict_Effect()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"gisaid_variants_statistics.json\",encoding=\"utf-8\", orient='records')\n",
    "dataset=[]\n",
    "db=pd.DataFrame(columns=['Time', 'Species', 'AA_Substitution', 'Lineage'])\n",
    "df=df.rename_axis('time').reset_index()\n",
    "spikeset=set()\n",
    "for i in range(df.index[-1]):\n",
    "    country=str(df.iloc[i][\"stats\"])[2:str(df.iloc[i][\"stats\"]).find(\":\")-1]\n",
    "    df.iloc[i][\"stats\"][country]\n",
    "    db.loc[len(db.index)]=(int(str(df.iloc[i][\"time\"])[:4])-2010)*365+int(str(df.iloc[i][\"time\"])[5:7])*30+int(str(df.iloc[i][\"time\"])[8:10]),df.iloc[i][\"stats\"][country]['submissions_per_variant'][0]['value'],df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'],df.iloc[i][\"stats\"][country]['submissions_per_lineage']\n",
    "    if len(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'])>1:\n",
    "        db.loc[i,'AA_Substitution']=['']\n",
    "        for j in range(int(len(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution']))):\n",
    "            db.iloc[i]['AA_Substitution'].append(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'][j]['value'])\n",
    "            spikeset.add(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'][j]['value'])\n",
    "        db.loc[i,'AA_Substitution'].remove('')\n",
    "    elif len(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'])==1:\n",
    "        db.loc[i,'AA_Substitution']=['']\n",
    "        db.iloc[i]['AA_Substitution'].append(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'][0]['value'])\n",
    "        spikeset.add(df.iloc[i][\"stats\"][country]['submissions_per_aa_substitution'][0]['value'])\n",
    "        db.loc[i,'AA_Substitution'].remove('')\n",
    "    if len(df.iloc[i][\"stats\"][country]['submissions_per_lineage'])>1:\n",
    "        db.loc[i,'Lineage']=['']\n",
    "        for j in range(int(len(df.iloc[i][\"stats\"][country]['submissions_per_lineage']))):\n",
    "            db.iloc[i]['Lineage'].append(df.iloc[i][\"stats\"][country]['submissions_per_lineage'][j]['value'])\n",
    "        db.loc[i,'Lineage'].remove('')\n",
    "    elif len(df.iloc[i][\"stats\"][country]['submissions_per_lineage'])==1:\n",
    "        db.loc[i,'Lineage']=['']\n",
    "        db.iloc[i]['Lineage'].append(df.iloc[i][\"stats\"][country]['submissions_per_lineage'][0]['value'])\n",
    "        db.loc[i,'Lineage'].remove('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "y = list(spikeset)\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "ylist=[]\n",
    "for i in range(db.values.T[2].shape[0]):\n",
    "    ylist.append(list(le.transform(db.values.T[2][i])))\n",
    "ylistarr=np.zeros((len(ylist),len(list(spikeset))))\n",
    "for i in range (len(ylist)):\n",
    "    for j in range(len(list(spikeset))):\n",
    "        if j in ylist[i]:\n",
    "            ylistarr[i,j]=1\n",
    "data_x = db.values.T[0].astype(np.float32).reshape(-1,1)\n",
    "data_y = ylistarr.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "accuracy 0.6842424242424242\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_94 (Dense)            (None, 1000)              2000      \n",
      "                                                                 \n",
      " dropout_63 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dropout_64 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,053,050\n",
      "Trainable params: 1,053,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(data_x,data_y,test_size=0.2)\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(units=1000,kernel_initializer='normal',activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units=1000,kernel_initializer='normal',activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units=data_y.shape[1],kernel_initializer='normal',activation='softmax'))\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "\n",
    "train_history=model2.fit(x=xtrain,y=ytrain,validation_split=0.2,epochs=20,batch_size=200,verbose=0)\n",
    "xpred=model.predict(xtest)\n",
    "accuracy=0\n",
    "for i in range(ytest.shape[0]):  #Rewrite accuracy function\n",
    "    for j in range(ytest.shape[1]):\n",
    "        if ytest[i][j]==xpred[i][j]:\n",
    "            accuracy+=1\n",
    "accuracy=accuracy/ytest.shape[0]/ytest.shape[1] \n",
    "print('accuracy',accuracy)\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "['Spike_K444N', 'Spike_L368I', 'Spike_T478K', 'Spike_R346T', 'Spike_S373P', 'Spike_T376A', 'Spike_P681H', 'Spike_G339D', 'Spike_S375F', 'Spike_D614G', 'Spike_Q498R']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "timestr=\"\"\n",
    "if len(timestr)!=10:\n",
    "    timestr=input(\"Enter the time that you want to predict(Ex: 2023.01.01): \")\n",
    "    if len(timestr)!=10:\n",
    "        print(\"Invalid input, please check and enter again!\")\n",
    "time=float((int(timestr[:4])-2010)*365+int(timestr[5:7])*30+int(timestr[8:10]))\n",
    "pred=model2.predict(np.array(time).reshape(-1,1))[0]\n",
    "spike=[]\n",
    "for i in range(pred.shape[0]):\n",
    "    if math.log10(pred[i])>-10:\n",
    "        spike.append(list(spikeset)[i])\n",
    "print(spike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=open('Mutations and Evolution of the SARS-CoV-2 Spike Protein.txt','r',encoding=\"utf-8\")\n",
    "document=txt.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T478\n",
      "P681\n",
      "G339\n",
      "D614\n",
      "Q498\n"
     ]
    }
   ],
   "source": [
    "global sentence\n",
    "sentence=[]\n",
    "for i in range(len(spike)):\n",
    "    if document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])!=-1:\n",
    "        print(str(spike[i])[str(spike[i]).find(\"_\")+1:10])\n",
    "        sentence.append(document[document[:document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])].rfind(\".\")+2:document[document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:]):].find(\".\")+document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The possible property of the new virus is(are): Immune escape capability, Rapidly dissemination ability, "
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "txt=open('Mutations and Evolution of the SARS-CoV-2 Spike Protein.txt','r',encoding=\"utf-8\")\n",
    "document=txt.read()\n",
    "global sentence\n",
    "sentence=[]\n",
    "for i in range(len(spike)):\n",
    "    if document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])!=-1:\n",
    "        print(str(spike[i])[str(spike[i]).find(\"_\")+1:10])\n",
    "        sentence.append(document[document[:document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])].rfind(\".\")+2:document[document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:]):].find(\".\")+document.find(str(spike[i])[str(spike[i]).find(\"_\")+1:10])])\n",
    "cutwords5=[]\n",
    "for i in range (len(sentence)):\n",
    "    cutwords1 = word_tokenize(sentence[i])\n",
    "    interpunctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "    cutwords2 = [word for word in cutwords1 if word not in interpunctuations]\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    cutwords3 = [word for word in cutwords2 if word not in stops]\n",
    "    cutwords4 = []\n",
    "    for cutword in cutwords3:\n",
    "        cutwords4.append(PorterStemmer().stem(cutword))\n",
    "    cutwords5.append(cutwords4)\n",
    "vocab=[['neutral','diminish','evad','impair','immune-evas','resist','lost-act','escap'],['mouse-adapt','mous'],['infect','bind','viral-load','receptor-bind','receptor','transmiss','spread'],['destabil']]\n",
    "#List1:Immune escape capability; 2:Zoonosis 3.Rapidly dissemination ability 4.Mutability\n",
    "Immune_escape_capability=Zoonosis=Rapidly_dissemination_ability=Mutability=count=0\n",
    "for i in range(len(cutwords5)):\n",
    "    for j in range(len(cutwords5[i])):\n",
    "        if cutwords5[i][j] in vocab[0]:\n",
    "            Immune_escape_capability+=1\n",
    "        if cutwords5[i][j] in vocab[1]:\n",
    "            Zoonosis+=1\n",
    "        if cutwords5[i][j] in vocab[2]:\n",
    "            Rapidly_dissemination_ability+=1\n",
    "        if cutwords5[i][j] in vocab[3]:\n",
    "            Mutability+=1\n",
    "print(\"The possible property of the new virus is(are): \",end=\"\")\n",
    "if Immune_escape_capability>0:\n",
    "    print(str(Immune_escape_capability)+\" times of immune escape capability, \",end=\"\")\n",
    "if Zoonosis>0:\n",
    "    print(str(Zoonosis)+\" times of zoonosis, \",end=\"\")\n",
    "if Rapidly_dissemination_ability>0:\n",
    "    print(str(Rapidly_dissemination_ability)+\" times of rapidly dissemination ability, \",end=\"\")\n",
    "if Mutability>0:\n",
    "    print(str(Mutability)+\" times of mutability, \",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Spike_N440K', 'Spike_D796Y', 'Spike_D405N', 'Spike_R408S', 'Spike_K444N', 'Spike_N460K', 'Spike_L368I', 'Spike_G142D', 'Spike_T478K', 'Spike_Y145del', 'Spike_G339H', 'Spike_F486P', 'N_E136D', 'Spike_S255F', 'Spike_G257S', 'Spike_D253G', 'Spike_W152R', 'Spike_F490S', 'Spike_K147E', 'Spike_R346T', 'Spike_K356T', 'Spike_S371F', 'Spike_S373P', 'Spike_S494P', 'Spike_V445P', 'Spike_G446S', 'Spike_L452R', 'Spike_E484A', 'Spike_V70del', 'N_P13L', 'Spike_T376A', 'Spike_P251L', 'Spike_P681H', 'Spike_F486S', 'Spike_H655Y', 'Spike_S477N', 'Spike_Y144del', 'Spike_N501Y', 'NS7a_P84L', 'Spike_H69del', 'Spike_G339D', 'Spike_K417N', 'Spike_K444T', 'Spike_S375F', 'Spike_Y505H', 'Spike_D614G', 'Spike_Q498R', 'E_T11A', 'Spike_H146Q', 'Spike_F486V'}\n"
     ]
    }
   ],
   "source": [
    "print(spikeset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mous'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"mouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
